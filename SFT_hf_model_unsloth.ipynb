{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune `meta-llama/Meta-Llama-3-8B-Instruct` on an EC2 instance using `Unsloth`\n",
    "---\n",
    "\n",
    "Unsloth makes finetuning large language models like Llama-3, Mistral, Phi-4 and Gemma 2x faster, use 70% less memory, and with no degradation in accuracy!\n",
    "\n",
    "**Note**: ***This notebook is run on a `g6e.12xlarge` instance. Follow the prerequisite steps [here](README.md)***\n",
    "\n",
    "In this example, we will be fine tuning the llama3 8b instruct model. There are several 4bit pre quantized models that `unsloth` provides that are not gated. This supports 4x faster downloading with no OOMs. In this case, we will be using the standard `meta-llama/Meta-Llama-3-8B-Instruct` model from hugging face. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/repos/unsloth-finetune-ec2/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 19:53:37,179\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import globals as g\n",
    "from dotenv import load_dotenv\n",
    "from unsloth import to_sharegpt\n",
    "from datasets import load_dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth import standardize_sharegpt\n",
    "from ec2_metrics import EC2MetricsCallback\n",
    "\n",
    "# Create a logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Remove existing handlers\n",
    "logger.handlers.clear()\n",
    "\n",
    "# Add a simple handler\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-24 20:04:12,307] p14838 {476936859.py:11} INFO - hf_model_id=meta-llama/Llama-3.1-8B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "import getpass\n",
    "load_dotenv()\n",
    "if not os.getenv(\"HF_TOKEN\"):\n",
    "    os.environ[\"HF_TOKEN\"] = getpass.getpass(\"Enter your HuggingFace token: \")\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if not os.getenv(\"HF_MODEL_ID\"):\n",
    "    hf_model_id  = input(\"Enter the model id to use for fine-tuning (e.g. meta-llama/Llama-3.1-8B-Instruct): \")\n",
    "\n",
    "logger.info(f\"hf_model_id={hf_model_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
    "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "DATASET_OF_INTEREST: str = 'vicgalle/alpaca-gpt4'\n",
    "\n",
    "ALPACA_PROMPT: str = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA L40S. Max memory: 44.309 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.25it/s]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = hf_model_id,\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        token = hf_token # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error occurred while loading the model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.15 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "\n",
    "We now use the Alpaca dataset from vicgalle, which is a version of 52K of the original Alpaca dataset generated from GPT4. You can replace this code section with your own data prep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-24 19:55:36,888] p14838 {2022229014.py:2} INFO - Columns in the dataset: ['instruction', 'input', 'output', 'text']\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(DATASET_OF_INTEREST, split=\"train\")\n",
    "logger.info(f\"Columns in the dataset: {dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = to_sharegpt(\n",
    "    dataset,\n",
    "    merged_prompt=\"{instruction}[[\\nYour input is:\\n{input}]]\",\n",
    "    output_column_name=\"output\",\n",
    "    conversation_extension=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the standardize_sharegpt function to just make the dataset in a correct format for finetuning\n",
    "dataset = standardize_sharegpt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversations': [[{'content': 'Give three tips for staying healthy.',\n",
      "                     'role': 'user'},\n",
      "                    {'content': '1. Eat a balanced and nutritious diet: Make '\n",
      "                                'sure your meals are inclusive of a variety of '\n",
      "                                'fruits and vegetables, lean protein, whole '\n",
      "                                'grains, and healthy fats. This helps to '\n",
      "                                'provide your body with the essential '\n",
      "                                'nutrients to function at its best and can '\n",
      "                                'help prevent chronic diseases.\\n'\n",
      "                                '\\n'\n",
      "                                '2. Engage in regular physical activity: '\n",
      "                                'Exercise is crucial for maintaining strong '\n",
      "                                'bones, muscles, and cardiovascular health. '\n",
      "                                'Aim for at least 150 minutes of moderate '\n",
      "                                'aerobic exercise or 75 minutes of vigorous '\n",
      "                                'exercise each week.\\n'\n",
      "                                '\\n'\n",
      "                                '3. Get enough sleep: Getting enough quality '\n",
      "                                'sleep is crucial for physical and mental '\n",
      "                                'well-being. It helps to regulate mood, '\n",
      "                                'improve cognitive function, and supports '\n",
      "                                'healthy growth and immune function. Aim for '\n",
      "                                '7-9 hours of sleep each night.',\n",
      "                     'role': 'assistant'},\n",
      "                    {'content': 'Describe what a monotheistic religion is.',\n",
      "                     'role': 'user'},\n",
      "                    {'content': 'A monotheistic religion is a type of religion '\n",
      "                                'that believes in the existence of only one '\n",
      "                                'supreme and all-powerful deity, who is '\n",
      "                                'considered the creator and ruler of the '\n",
      "                                'universe. This deity is worshiped as the '\n",
      "                                'ultimate and only divine being, and followers '\n",
      "                                'of such religions often see their deity as '\n",
      "                                'omniscient, omnipotent, and omnibenevolent. '\n",
      "                                'Some of the most widely practiced '\n",
      "                                'monotheistic religions in the world today '\n",
      "                                'include Christianity, Islam, and Judaism, '\n",
      "                                'among others. The concept of monotheism '\n",
      "                                'differs from polytheism, which believes in '\n",
      "                                'the existence of multiple gods, and from '\n",
      "                                'atheism, which denies the existence of any '\n",
      "                                'deity.',\n",
      "                     'role': 'assistant'},\n",
      "                    {'content': 'How does one add a chart to a document?',\n",
      "                     'role': 'user'},\n",
      "                    {'content': 'To add a chart to a document, follow these '\n",
      "                                'steps:\\n'\n",
      "                                '\\n'\n",
      "                                '1. Open the document where you want to insert '\n",
      "                                'the chart.\\n'\n",
      "                                '2. Click the location where you want to '\n",
      "                                'insert the chart.\\n'\n",
      "                                '3. In most word processors, you can go to the '\n",
      "                                '**Insert** tab, where you can find a '\n",
      "                                '**Chart** option. Click on it.\\n'\n",
      "                                \"4. A new window will appear, and you'll be \"\n",
      "                                'able to select the chart type that you want '\n",
      "                                'to use (column, pie, line, bar, area, '\n",
      "                                'scatter, etc.).\\n'\n",
      "                                \"5. Once you've selected your chart type, \"\n",
      "                                'youâ€™ll be prompted to enter your data into a '\n",
      "                                'spreadsheet. You can either type your data in '\n",
      "                                'manually or copy it from an existing data '\n",
      "                                'source.\\n'\n",
      "                                '6. Edit your chart data and customize its '\n",
      "                                \"appearance to fit your document's style or \"\n",
      "                                'branding.\\n'\n",
      "                                '7. When youâ€™re happy with how the chart '\n",
      "                                'looks, click \"OK\" or \"Insert\" to add the '\n",
      "                                'chart to your document.\\n'\n",
      "                                '8. Optionally, you can add a chart title or '\n",
      "                                'labels to the axes to make the chart easier '\n",
      "                                'to understand.\\n'\n",
      "                                '\\n'\n",
      "                                'These instructions may vary depending on the '\n",
      "                                'type of word processor you are using.',\n",
      "                     'role': 'assistant'}],\n",
      "                   [{'content': 'What are the three primary colors?',\n",
      "                     'role': 'user'},\n",
      "                    {'content': 'The three primary colors are red, blue, and '\n",
      "                                'yellow. These colors are called primary '\n",
      "                                'because they cannot be created by mixing '\n",
      "                                'other colors and all other colors can be made '\n",
      "                                'by combining them in various proportions. In '\n",
      "                                'the additive color system, used for light, '\n",
      "                                'the primary colors are red, green, and blue '\n",
      "                                '(RGB).',\n",
      "                     'role': 'assistant'},\n",
      "                    {'content': 'Identify 3 different features of the given '\n",
      "                                'product.\\n'\n",
      "                                'Your input is:\\n'\n",
      "                                'Apple Watch Series 6',\n",
      "                     'role': 'user'},\n",
      "                    {'content': 'Three features of the Apple Watch Series 6 '\n",
      "                                'are:\\n'\n",
      "                                '\\n'\n",
      "                                '1. Blood Oxygen Monitoring: The Apple Watch '\n",
      "                                'Series 6 includes a Blood Oxygen sensor that '\n",
      "                                'measures the oxygen saturation (SpO2) of the '\n",
      "                                'wearerâ€™s blood. It uses infrared and red LEDs '\n",
      "                                'to take measurements, alongside photodiodes '\n",
      "                                'that measure the light reflected from the '\n",
      "                                \"blood in the user's veins.\\n\"\n",
      "                                '\\n'\n",
      "                                '2. Always-On Retina Display: The Always-On '\n",
      "                                'Retina display ensures that the watch face is '\n",
      "                                'always visible, even when the userâ€™s wrist is '\n",
      "                                'down. This makes it easier to check the time, '\n",
      "                                'notifications, and other important '\n",
      "                                'information without having to raise your '\n",
      "                                'wrist or tap on the screen.\\n'\n",
      "                                '\\n'\n",
      "                                '3. Fitness Tracking: The Apple Watch Series 6 '\n",
      "                                'has a range of features designed to help '\n",
      "                                'users track their fitness and stay active. '\n",
      "                                'These include tracking workouts, monitoring '\n",
      "                                'daily activity levels, and setting reminders '\n",
      "                                'to stand or move around periodically. The '\n",
      "                                'watch also includes a heart rate monitor to '\n",
      "                                'track cardio fitness levels during exercise.',\n",
      "                     'role': 'assistant'},\n",
      "                    {'content': 'Given a description of a character, come up '\n",
      "                                \"with possible motivations for the character's \"\n",
      "                                'behaviour.\\n'\n",
      "                                'Your input is:\\n'\n",
      "                                'The character is a young man who is often '\n",
      "                                'confrontational and rebellious.',\n",
      "                     'role': 'user'},\n",
      "                    {'content': '1. The young man may have grown up in a '\n",
      "                                'challenging home environment where he had to '\n",
      "                                'fight to be heard, leading him to develop '\n",
      "                                'confrontational behaviors.\\n'\n",
      "                                '\\n'\n",
      "                                '2. He may be struggling with feelings of '\n",
      "                                'insecurity or low self-worth, leading him to '\n",
      "                                'act out in rebellious ways to gain attention '\n",
      "                                'or assert his individuality.\\n'\n",
      "                                '\\n'\n",
      "                                '3. The young man may have experienced a '\n",
      "                                'traumatic event or loss, causing him to feel '\n",
      "                                'anger and resentment towards the world, '\n",
      "                                'leading him to behave confrontationally.\\n'\n",
      "                                '\\n'\n",
      "                                '4. He may feel misunderstood or stifled by '\n",
      "                                'societal expectations, leading him to rebel '\n",
      "                                'against norms and conventions as a means of '\n",
      "                                'expressing his frustration.\\n'\n",
      "                                '\\n'\n",
      "                                '5. The young man may be searching for a sense '\n",
      "                                'of identity or purpose, causing him to engage '\n",
      "                                'in confrontational behavior as a way of '\n",
      "                                'defining and asserting himself.\\n'\n",
      "                                '\\n'\n",
      "                                '6. He may be influenced by a group of peers '\n",
      "                                'who engage in rebellious behavior, causing '\n",
      "                                'him to adopt similar behaviors in order to '\n",
      "                                'fit in with the group.',\n",
      "                     'role': 'assistant'}],\n",
      "                   [{'content': 'Describe the structure of an atom.',\n",
      "                     'role': 'user'},\n",
      "                    {'content': 'An atom is the basic building block of all '\n",
      "                                'matter and is made up of three types of '\n",
      "                                'particles: protons, neutrons, and electrons. '\n",
      "                                'The structure of an atom can be described as '\n",
      "                                'a nucleus at the center surrounded by a cloud '\n",
      "                                'of electrons.\\n'\n",
      "                                '\\n'\n",
      "                                'The nucleus of an atom is made up of protons '\n",
      "                                'and neutrons. Protons are positively charged '\n",
      "                                'particles and neutrons are neutral particles '\n",
      "                                'with no charge. Both of these particles are '\n",
      "                                'located in the nucleus of the atom, which is '\n",
      "                                'at the center of the atom and contains most '\n",
      "                                \"of the atom's mass.\\n\"\n",
      "                                '\\n'\n",
      "                                'Surrounding the nucleus of the atom is a '\n",
      "                                'cloud of electrons. Electrons are negatively '\n",
      "                                'charged particles that are in constant motion '\n",
      "                                'around the nucleus. The electron cloud is '\n",
      "                                'divided into shells or orbitals, and each '\n",
      "                                'shell can hold a certain number of electrons. '\n",
      "                                'The number of electrons in the outermost '\n",
      "                                'shell, called the valence shell, determines '\n",
      "                                'the chemical properties of the atom. \\n'\n",
      "                                '\\n'\n",
      "                                'In a neutral atom, the number of protons in '\n",
      "                                'the nucleus is equal to the number of '\n",
      "                                'electrons in the electron cloud, so the '\n",
      "                                'positive and negative charges balance out and '\n",
      "                                'the atom has no overall charge. The number of '\n",
      "                                'protons, also called the atomic number, '\n",
      "                                'determines what element the atom is.',\n",
      "                     'role': 'assistant'},\n",
      "                    {'content': 'Describe the setting of the book \"Alice in '\n",
      "                                'Wonderland\".',\n",
      "                     'role': 'user'},\n",
      "                    {'content': 'The setting of the book \"Alice in Wonderland\" '\n",
      "                                'is primarily in a fantasy world located in a '\n",
      "                                'rabbit hole that Alice, the main character, '\n",
      "                                'falls into. This surreal world is called '\n",
      "                                'Wonderland and is filled with strange '\n",
      "                                'anthropomorphic creatures, magical '\n",
      "                                'occurrences, and illogical events. The '\n",
      "                                'environment changes from one scene to '\n",
      "                                'another, including lush gardens, forests, and '\n",
      "                                'the royal courtyard of the Queen of Hearts. '\n",
      "                                'As Alice moves through the different parts of '\n",
      "                                'Wonderland, she encounters unusual and '\n",
      "                                'puzzling situations, like changing in size, '\n",
      "                                'attending an unusual tea party, and partaking '\n",
      "                                'in a nonsensical trial. The setting of the '\n",
      "                                'book is dream-like and whimsical, with a '\n",
      "                                'touch of illogic and absurdity.',\n",
      "                     'role': 'assistant'},\n",
      "                    {'content': 'Rearrange the words given in the input to '\n",
      "                                'make a meaningful sentence.\\n'\n",
      "                                'Your input is:\\n'\n",
      "                                'society a modern in work importance gender of',\n",
      "                     'role': 'user'},\n",
      "                    {'content': 'The importance of gender in modern society '\n",
      "                                'work.',\n",
      "                     'role': 'assistant'}]]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(dataset[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: We automatically added an EOS token to stop endless generations.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52002/52002 [00:04<00:00, 12890.01 examples/s]\n"
     ]
    }
   ],
   "source": [
    "chat_template = \"\"\"Below are some instructions that describe some tasks. Write responses that appropriately complete each request.\n",
    "\n",
    "### Instruction:\n",
    "{INPUT}\n",
    "\n",
    "### Response:\n",
    "{OUTPUT}\"\"\"\n",
    "\n",
    "from unsloth import apply_chat_template\n",
    "\n",
    "dataset = apply_chat_template(\n",
    "    dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    chat_template=chat_template,\n",
    "    # default_system_message = \"You are a helpful assistant\", << [OPTIONAL]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting train dataset to ChatML (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52002/52002 [00:02<00:00, 20855.96 examples/s]\n",
      "Applying chat template to train dataset (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52002/52002 [00:04<00:00, 12355.27 examples/s]\n",
      "Tokenizing train dataset (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52002/52002 [00:28<00:00, 1845.10 examples/s]\n",
      "Tokenizing train dataset (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52002/52002 [00:12<00:00, 4201.02 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.32 s, sys: 486 ms, total: 1.81 s\n",
      "Wall time: 48.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train the model\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        # num_train_epochs = 1, # For longer training runs!\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    "    callbacks=[EC2MetricsCallback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 52,002 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n",
      "[2025-02-24 19:57:09,764] p14838 {ec2_metrics.py:184} INFO - Training started. Initiating EC2 metrics collection.\n",
      "[2025-02-24 19:57:09,765] p14838 {ec2_metrics.py:170} INFO - Writing header: ['timestamp', 'cpu_percent_mean', 'memory_percent_mean', 'memory_used_mean', 'gpu_utilization_mean', 'gpu_memory_used_mean', 'gpu_memory_free_mean', 'gpu_memory_total_mean']\n",
      "[2025-02-24 19:57:09,765] p14838 {ec2_metrics.py:41} INFO - Starting collection\n",
      "[2025-02-24 19:57:10,117] p14838 {ec2_metrics.py:143} INFO - Starting daemon collector to run in background\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:46, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.446200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.436200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.390700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.485800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.569000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.311700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.335600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.326200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.342500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.228900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.270500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.287100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.089900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.138700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.130500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.073900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.072500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.214700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.130200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.082900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.053500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.106700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.066400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.048000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.078400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.101300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.081800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.069100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.147700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.095800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.087900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.985100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.076200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.158900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.047300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.987200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.066500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.158300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.007200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.054200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.073100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.141700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.043700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.216500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.095500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.119500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.075300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.992000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.009200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.051600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.035700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.997000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.023900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-24 19:59:01,247] p14838 {ec2_metrics.py:191} INFO - Training ended. Stopping EC2 metrics collection.\n",
      "[2025-02-24 19:59:01,247] p14838 {ec2_metrics.py:33} INFO - Stopped collection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 31s, sys: 25.9 s, total: 1min 57s\n",
      "Wall time: 1min 52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-24 19:59:05,119] p14838 {ec2_metrics.py:33} INFO - Stopped collection\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# this will initiate the training process and also log the EC2 utilization metrics, such as the GPU\n",
    "# utilization, CPU utilization, etc.\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log the trainer stats\n",
    "---\n",
    "\n",
    "In this step, we log some of the trainer stats, such as the number of global steps it took to get to a specific training loss, the train runtime, samples per second, steps per second, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the training stats in a readable way\n",
    "output_text = f\"\"\"Training Statistics:\n",
    "Global Steps: {trainer_stats.global_step}\n",
    "Training Loss: {trainer_stats.training_loss:.4f}\n",
    "\n",
    "Metrics:\n",
    "- Train Runtime: {trainer_stats.metrics['train_runtime']:.3f} seconds\n",
    "- Training Samples/Second: {trainer_stats.metrics['train_samples_per_second']:.3f}\n",
    "- Training Steps/Second: {trainer_stats.metrics['train_steps_per_second']:.3f}\n",
    "- Total FLOPS: {trainer_stats.metrics['total_flos']:.2e}\n",
    "- Final Train Loss: {trainer_stats.metrics['train_loss']:.4f}\n",
    "\"\"\"\n",
    "\n",
    "# Save to a text file\n",
    "with open(os.path.join(g.RESULTS_DIR, g.TRAINING_STATS), 'w') as f:\n",
    "    f.write(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next number in the Fibonacci sequence is 13.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "messages = [                    # Change below!\n",
    "    {\"role\": \"user\", \"content\": \"Continue the fibonacci sequence! Your input is 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tallest tower in France is called the Eiffel Tower.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "messages = [                         # Change below!\n",
    "    {\"role\": \"user\",      \"content\": \"Continue the fibonacci sequence! Your input is 1, 1, 2, 3, 5, 8\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The fibonacci sequence continues as 13, 21, 34, 55 and 89.\"},\n",
    "    {\"role\": \"user\",      \"content\": \"What is France's tallest tower called?\"},\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama Support [Optional]\n",
    "\n",
    "Unsloth now allows you to automatically finetune and create a Modelfile, and export to Ollama! This makes finetuning much easier and provides a seamless workflow from Unsloth to Ollama!\n",
    "\n",
    "Let's first install Ollama!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "######################################################################## 100.0%#############################                                59.5%####################################################           88.4%\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to render group...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      ">>> Enabling and starting ollama service...\n",
      "Created symlink /etc/systemd/system/default.target.wants/ollama.service â†’ /etc/systemd/system/ollama.service.\n",
      ">>> NVIDIA GPU installed.\n"
     ]
    }
   ],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: ##### The current model auto adds a BOS token.\n",
      "Unsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n",
      "Makefile:2: *** The Makefile build is deprecated. Use the CMake build instead. For more details, see https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md.  Stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/home/ubuntu/repos/unsloth-finetune-ec2/llama.cpp'\n",
      "make: Leaving directory '/home/ubuntu/repos/unsloth-finetune-ec2/llama.cpp'\n",
      "-- The C compiler identification is GNU 11.4.0\n",
      "-- The CXX compiler identification is GNU 11.4.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Found Git: /usr/bin/git (found version \"2.34.1\") \n",
      "-- Looking for pthread.h\n",
      "-- Looking for pthread.h - found\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE  \n",
      "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
      "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "-- Including CPU backend\n",
      "-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n",
      "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
      "-- Found OpenMP: TRUE (found version \"4.5\")  \n",
      "-- x86 detected\n",
      "-- Adding CPU backend variant ggml-cpu: -march=native \n",
      "-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")  \n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /home/ubuntu/repos/unsloth-finetune-ec2/llama.cpp/build\n",
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 300.22 out of 372.73 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:01<00:00, 23.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Converting llama model. Can use fast conversion = False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q8_0'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: CMAKE detected. Finalizing some steps for installation.\n",
      "Unsloth: [1] Converting model at model into q8_0 GGUF format.\n",
      "The output location will be /home/ubuntu/repos/unsloth-finetune-ec2/model/unsloth.Q8_0.gguf\n",
      "This might take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: model\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> Q8_0, shape = {4096, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 14336}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --> Q8_0, shape = {4096, 1024}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:output.weight,               torch.bfloat16 --> Q8_0, shape = {4096, 128256}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --> Q8_0, shape = {14336, 4096}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {4096}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 14336\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 7\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128009\n",
      "INFO:gguf.vocab:Setting special token type pad to 128004\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.vocab:Setting chat_template to {{ 'Below are some instructions that describe some tasks. Write responses that appropriately complete each request.' }}{% for message in messages %}{% if message['role'] == 'user' %}{{ '\n",
      "\n",
      "### Instruction:\n",
      "' + message['content'] }}{% elif message['role'] == 'assistant' %}{{ '\n",
      "\n",
      "### Response:\n",
      "' + message['content'] + '<|eot_id|>' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '\n",
      "\n",
      "### Response:\n",
      "' }}{% endif %}\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/home/ubuntu/repos/unsloth-finetune-ec2/model/unsloth.Q8_0.gguf: n_tensors = 292, total_size = 8.5G\n",
      "Writing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.53G/8.53G [00:50<00:00, 168Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to /home/ubuntu/repos/unsloth-finetune-ec2/model/unsloth.Q8_0.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: ##### The current model auto adds a BOS token.\n",
      "Unsloth: ##### We removed it in GGUF's chat template for you.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Conversion completed! Output location: /home/ubuntu/repos/unsloth-finetune-ec2/model/unsloth.Q8_0.gguf\n",
      "Unsloth: Saved Ollama Modelfile to model/Modelfile\n"
     ]
    }
   ],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if True: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: listen tcp 127.0.0.1:11434: bind: address already in use\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.Popen([\"ollama\", \"serve\"])\n",
    "import time\n",
    "\n",
    "time.sleep(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lgathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 0% â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 1% â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 2% â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 3% â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 4% â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 4% â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 5% â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 6% â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 7% â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 7% â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 8% â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 9% â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 10% â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 11% â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 12% â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 13% â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 13% â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 14% â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 15% â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 16% â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 16% â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 17% â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 18% â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 19% â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 20% â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 21% â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 21% â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 22% â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 23% â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 24% â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 25% â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 26% â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 26% â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 27% â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 28% â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 29% â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 30% â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 31% â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 31% â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 32% â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 33% â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 34% â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 35% â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 36% â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 36% â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 37% â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 38% â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 39% â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 40% â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 41% â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 41% â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 42% â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 43% â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 44% â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 45% â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 46% â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 46% â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 47% â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 48% â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 49% â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 50% â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 51% â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 51% â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 52% â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 53% â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 54% â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 55% â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 56% â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 56% â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 57% â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 58% â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 59% â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 60% â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 61% â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 61% â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 62% â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 63% â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 64% â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 65% â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 66% â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 66% â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 67% â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 68% â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 69% â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 70% â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 71% â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 71% â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 72% â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 73% â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 74% â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 75% â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 76% â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 76% â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 77% â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 78% â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 79% â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 80% â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 81% â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 81% â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 82% â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 83% â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 84% â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 85% â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 86% â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 86% â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 87% â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 88% â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 89% â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 90% â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 90% â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 91% â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 92% â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 93% â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 93% â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 94% â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 95% â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 95% â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 96% â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 97% â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 97% â  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 98% â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 99% â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 100% \n",
      "parsing GGUF \n",
      "using existing layer sha256:e962cf834f9f8cca4b22ba8d60c1c9d5407b32d7f4c644fa42698614b8434a23 \n",
      "creating new layer sha256:a171a8dc44d851d6f1a33ddcbe8af761f9c903bb55706d775ed46d0e1e6cccfe \n",
      "creating new layer sha256:078102b4a4240b8a3d083d9e8d0c75c3abb842a813e977dcd178941072b4284b \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama create unsloth_model -f ./model/Modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model\":\"unsloth_model\",\"created_at\":\"2025-02-24T19:51:41.931380934Z\",\"message\":{\"role\":\"assistant\",\"content\":\"13\"},\"done\":false}\n",
      "{\"model\":\"unsloth_model\",\"created_at\":\"2025-02-24T19:51:41.956113509Z\",\"message\":{\"role\":\"assistant\",\"content\":\",\"},\"done\":false}\n",
      "{\"model\":\"unsloth_model\",\"created_at\":\"2025-02-24T19:51:41.969893113Z\",\"message\":{\"role\":\"assistant\",\"content\":\" \"},\"done\":false}\n",
      "{\"model\":\"unsloth_model\",\"created_at\":\"2025-02-24T19:51:41.983350469Z\",\"message\":{\"role\":\"assistant\",\"content\":\"21\"},\"done\":false}\n",
      "{\"model\":\"unsloth_model\",\"created_at\":\"2025-02-24T19:51:41.996852607Z\",\"message\":{\"role\":\"assistant\",\"content\":\",\"},\"done\":false}\n",
      "{\"model\":\"unsloth_model\",\"created_at\":\"2025-02-24T19:51:42.010294123Z\",\"message\":{\"role\":\"assistant\",\"content\":\" \"},\"done\":false}\n",
      "{\"model\":\"unsloth_model\",\"created_at\":\"2025-02-24T19:51:42.023735629Z\",\"message\":{\"role\":\"assistant\",\"content\":\"34\"},\"done\":false}\n",
      "{\"model\":\"unsloth_model\",\"created_at\":\"2025-02-24T19:51:42.037175775Z\",\"message\":{\"role\":\"assistant\",\"content\":\",\"},\"done\":false}\n",
      "{\"model\":\"unsloth_model\",\"created_at\":\"2025-02-24T19:51:42.050637302Z\",\"message\":{\"role\":\"assistant\",\"content\":\" \"},\"done\":false}\n",
      "{\"model\":\"unsloth_model\",\"created_at\":\"2025-02-24T19:51:42.064071588Z\",\"message\":{\"role\":\"assistant\",\"content\":\"55\"},\"done\":false}\n",
      "{\"model\":\"unsloth_model\",\"created_at\":\"2025-02-24T19:51:42.077530145Z\",\"message\":{\"role\":\"assistant\",\"content\":\",\"},\"done\":false}\n",
      "{\"model\":\"unsloth_model\",\"created_at\":\"2025-02-24T19:51:42.091002091Z\",\"message\":{\"role\":\"assistant\",\"content\":\" \"},\"done\":false}\n",
      "{\"model\":\"unsloth_model\",\"created_at\":\"2025-02-24T19:51:42.104521999Z\",\"message\":{\"role\":\"assistant\",\"content\":\"89\"},\"done\":false}\n",
      "{\"model\":\"unsloth_model\",\"created_at\":\"2025-02-24T19:51:42.117934105Z\",\"message\":{\"role\":\"assistant\",\"content\":\",\"},\"done\":false}\n",
      "{\"model\":\"unsloth_model\",\"created_at\":\"2025-02-24T19:51:42.131389501Z\",\"message\":{\"role\":\"assistant\",\"content\":\" \"},\"done\":false}\n",
      "{\"model\":\"unsloth_model\",\"created_at\":\"2025-02-24T19:51:42.144883929Z\",\"message\":{\"role\":\"assistant\",\"content\":\"144\"},\"done\":false}\n",
      "{\"model\":\"unsloth_model\",\"created_at\":\"2025-02-24T19:51:42.158387416Z\",\"message\":{\"role\":\"assistant\",\"content\":\",\"},\"done\":false}\n",
      "{\"model\":\"unsloth_model\",\"created_at\":\"2025-02-24T19:51:42.171830782Z\",\"message\":{\"role\":\"assistant\",\"content\":\" ...\"},\"done\":false}\n",
      "{\"model\":\"unsloth_model\",\"created_at\":\"2025-02-24T19:51:42.188373007Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\"},\"done_reason\":\"stop\",\"done\":true,\"total_duration\":5300922739,\"load_duration\":4889889002,\"prompt_eval_count\":47,\"prompt_eval_duration\":150000000,\"eval_count\":19,\"eval_duration\":259000000}\n"
     ]
    }
   ],
   "source": [
    "# run inference against the model\n",
    "!curl http://localhost:11434/api/chat -d '{ \\\n",
    "    \"model\": \"unsloth_model\", \\\n",
    "    \"messages\": [ \\\n",
    "        { \"role\": \"user\", \"content\": \"Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8,\" } \\\n",
    "    ] \\\n",
    "    }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response generated: The next number in the Fibonacci sequence would be: 13\n"
     ]
    }
   ],
   "source": [
    "# run inference against the model\n",
    "import json\n",
    "result = subprocess.run(\n",
    "    [\n",
    "        \"curl\",\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        \"-d\",\n",
    "        '{\"model\": \"unsloth_model\", \"prompt\": \"Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8,\", \"stream\": false}',\n",
    "    ],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "response_data = json.loads(result.stdout)\n",
    "print(f\"Response generated: {response_data['response']}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
